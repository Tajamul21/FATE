{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TT_All_models_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K6NYTcAvln4J",
        "YHxUZWqils8I",
        "WF87tWVHk3l6",
        "OKEDxrId6U2r",
        "UcyUViM67jpb",
        "QCbok4hOM1tP",
        "RGHq5grV2epw",
        "HTCskCqW_jnE",
        "mIOhQSCaDBgI",
        "sRQXL-izB15z"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6NYTcAvln4J"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "**Requires dataset_tensor.npy file in \"Colab Notebooks/FATE/Data\" folder!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYQdKwENvZQo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/FATE/'\n",
        "DATA_PATH = PATH + 'Data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XNfw-0EfZ4B"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import sklearn.model_selection\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as kr\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "! pip install -q pyyaml h5py  # Required to save models in HDF5 format\n",
        "! pip install torch\n",
        "! pip install einops\n",
        "! pip install tqdm\n",
        "! pip install torchsummary\n",
        "! pip install scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHxUZWqils8I"
      },
      "source": [
        "### Clone FATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6Ej0xOjzbu"
      },
      "source": [
        "# Clone the entire repo.\n",
        "%cd /content\n",
        "!git clone -l -s https://github.com/onurbil/TENT.git tensorized_transformers\n",
        "%cd tensorized_transformers\n",
        "!ls\n",
        "%cd ..\n",
        "\n",
        "# Update the repository\n",
        "%cd tensorized_transformers\n",
        "!git pull\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmfy2j6FkjLo"
      },
      "source": [
        "import sys\n",
        "\n",
        "TT_REPO_PATH = '/content/tensorized_transformers'\n",
        "\n",
        "sys.path.append(TT_REPO_PATH)\n",
        "print(sys.path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF87tWVHk3l6"
      },
      "source": [
        "### Get datasets\n",
        "\n",
        "** Only if you don't have it saved in your drive **"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests \n",
        "import shutil\n",
        "import os\n",
        "from common.paths import PROCESSED_DATASET_DIR\n",
        "from common.paths import EU_PROCESSED_DATASET_DIR\n",
        "%cd /content\n",
        "%mkdir dataset\n",
        "%cd /content/dataset\n",
        "\n",
        "file_url = \"PASTE_SURFDRIVE_URL_HERE\"\n",
        "\n",
        "## Dowloading dataset\n",
        "try:\n",
        "  r = requests.get(file_url + '/download', stream = True) \n",
        "\n",
        "  with open(\"/content/dataset/TENT dataset.rar\", \"wb\") as file: \n",
        "    for block in r.iter_content(chunk_size = 1024): \n",
        "      if block: \n",
        "        file.write(block) \n",
        "\n",
        "except:\n",
        "  raise Exception(\"\\n_______________________________________________________\\nPlease request SurfDrive URL to: \\nsiamak.mehrkanoon@maastrichtuniversity.nl\\n\\nThen paste the URL given in the file_url variable on line 6\")\n",
        "\n",
        "# Unrar and change DATA_PATH \n",
        "!pip install unrar\n",
        "!unrar x 'TENT dataset.rar'\n",
        "\n",
        "\n",
        "filesToMoveUS = ['usa-canada_dataset_tensor.npy',\n",
        "               'usa-canada_scale.npy']\n",
        "filesToMoveEU = ['europe_dataset_tensor.npy',\n",
        "               'europe_scale.npy']\n",
        "\n",
        "os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)\n",
        "for files in filesToMoveUS:\n",
        "  shutil.copy(files, DATA_PATH)\n",
        "for files in filesToMoveEU:\n",
        "  shutil.copy(files, DATA_PATH)"
      ],
      "metadata": {
        "id": "0dRyVsiPx0Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twiSa_k66PPU"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "# Dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKEDxrId6U2r"
      },
      "source": [
        "## USA+Canada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ4NOY9-6cmx"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\n",
        "from common.variables import city_labels\n",
        "\n",
        "input_length = 16\n",
        "prediction_time = 4\n",
        "y_feature = 4\n",
        "y_city = 0\n",
        "num_cities = 30\n",
        "remove_last_from_test= 800 \n",
        "valid_size = 1024\n",
        "\n",
        "dataset, dataset_params = load_dataset.get_usa_dataset(DATA_PATH, \n",
        "                                                       input_length, prediction_time, \n",
        "                                                       y_feature, y_city, \n",
        "                                                       end_city=num_cities, \n",
        "                                                       remove_last_from_test=remove_last_from_test, \n",
        "                                                       valid_split=valid_size, split_random=1337)\n",
        "\n",
        "denorm_min, denorm_max = load_dataset.get_usa_normalization(DATA_PATH, y_feature)\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "print('Xtr.shape', Xtr.shape)\n",
        "print('Ytr.shape', Ytr.shape)\n",
        "print('Xvalid.shape', Xvalid.shape)\n",
        "print('Yvalid.shape', Yvalid.shape)\n",
        "print('Xtest.shape', Xtest.shape)\n",
        "print('Ytest.shape', Ytest.shape)\n",
        "\n",
        "print('denorm_min', denorm_min)\n",
        "print('denorm_max', denorm_max)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcyUViM67jpb"
      },
      "source": [
        "## EU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed4WFeoP7lvj"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\n",
        "from common.variables import eu_city_labels\n",
        "city_labels = eu_city_labels\n",
        "\n",
        "input_length = 8\n",
        "prediction_time = 6\n",
        "y_feature = 3  # 3=avg_temp(F)\n",
        "y_city = 1 \n",
        "valid_size = 512\n",
        "test_size = 1095 # 3 years of measurements\n",
        "\n",
        "dataset, dataset_params = load_dataset.get_eu_dataset(DATA_PATH, test_size,\n",
        "                                                      input_length, prediction_time, \n",
        "                                                      y_feature, y_city, \n",
        "                                                      valid_split=valid_size, split_random=1337)\n",
        "\n",
        "denorm_min, denorm_max = load_dataset.get_eu_normalization(DATA_PATH, y_feature)\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "print('Xtr.shape', Xtr.shape)\n",
        "print('Ytr.shape', Ytr.shape)\n",
        "print('Xvalid.shape', Xvalid.shape)\n",
        "print('Yvalid.shape', Yvalid.shape)\n",
        "print('Xtest.shape', Xtest.shape)\n",
        "print('Ytest.shape', Ytest.shape)\n",
        "\n",
        "print('denorm_min', denorm_min)\n",
        "print('denorm_max', denorm_max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NTfnysd6TDj"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "# Experiments\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCbok4hOM1tP"
      },
      "source": [
        "## Tensorized Transformer\n",
        "** Run on TPU **"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "import experiment_tools.tt_training as tt_training\n",
        "from visualization_tools.AW_save import save_weights\n",
        "import datetime\n",
        "\n",
        "save_aw = False ## To store the attention weights set this variable to true\n",
        "## Conf to load weights\n",
        "load_weights = True ## To store the attention weights set this variable to true\n",
        "load_name = city_labels[y_city].replace(\" \", \"\") + '_predtime_' + str(prediction_time) + '.h5'\n",
        "\n",
        "# Folders:\n",
        "folder = datetime.datetime.now().strftime(\"%Y%m%d\") + '_' + datetime.datetime.now().strftime(\"%H%M%S\")\n",
        "path_to_aw = '/content/drive/MyDrive/Colab Notebooks/FATE/AW/'\n",
        "model_save_path = path_to_aw + folder + '/model_' + folder + '.h5'\n",
        "\n",
        "# model\n",
        "softmax_type = 3\n",
        "epoch = 300\n",
        "patience = 20\n",
        "num_layers = 1\n",
        "head_num = 8\n",
        "d_model = 16\n",
        "dense_units = 32\n",
        "batch_size = 12\n",
        "loss = 'mse'\n",
        "\n",
        "if load_weights:\n",
        "  epoch = 0\n",
        "\n",
        "model, model_params, history = tt_training.train_model(dataset, \n",
        "                                                       softmax_type, epoch, patience, \n",
        "                                                       num_layers, head_num, d_model, dense_units, \n",
        "                                                       batch_size, loss, use_tpu=True, save_aw = save_aw)\n",
        "if save_aw:\n",
        "  save_weights(model, city_labels, layer=1, folder_name = path_to_aw + folder)\n",
        "  checkpoint = tf.train.Checkpoint(model=model)\n",
        "  local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "  model.save_weights(model_save_path, options=local_device_option)\n",
        "\n",
        "if load_weights:\n",
        "  model_load_path = TT_REPO_PATH + '/trained_models/' + load_name\n",
        "  checkpoint = tf.train.Checkpoint(model=model)\n",
        "  local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "  model.load_weights(model_load_path, options=local_device_option)\n",
        "\n",
        "end = time.time()\n",
        "print('Time elapsed: ', str(np.round(end - start, decimals=2)), 'seconds')"
      ],
      "metadata": {
        "id": "C6dQxTrdQgsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcJCcTSyK0Rn"
      },
      "source": [
        "import experiment_tools.results as results\n",
        "\n",
        "params = dataset_params + model_params\n",
        "results.print_params(params)\n",
        "\n",
        "folder, name = results.save_results_with_datetime(model, 'TT', PATH, params)\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "\n",
        "r = results.plot_valid_test_predictions(model, Xvalid, Yvalid, Xtest, Ytest, \n",
        "                                    y_feature=None, denorm_min=denorm_min, denorm_max=denorm_max, \n",
        "                                    folder=folder, base_name=name)\n",
        "print(f'{r[2]}\\t{r[3]}\\t{r[4]}\\t{r[5]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGHq5grV2epw"
      },
      "source": [
        "## Vanilla Transformer\n",
        "** Run on GPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLW9Jgh62epy"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "import experiment_tools.vanilla_training as vanilla_training\n",
        "\n",
        "# model\n",
        "epoch = 300\n",
        "patience = 20\n",
        "num_layers = 3 \n",
        "head_num = 32\n",
        "d_model = 512\n",
        "dense_units = 512\n",
        "dropout_rate = 0.01\n",
        "batch_size = 128\n",
        "loss = kr.losses.mean_squared_error\n",
        "use_tpu = False\n",
        "\n",
        "model, model_params = vanilla_training.train_model(dataset, \n",
        "                                                   epoch, patience,\n",
        "                                                   num_layers, head_num,\n",
        "                                                   d_model, dense_units,\n",
        "                                                   batch_size, dropout_rate,\n",
        "                                                   loss, use_tpu=use_tpu)\n",
        "\n",
        "end = time.time()\n",
        "print('Time elapsed: ', str(np.round(end - start, decimals=2)), 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es3bMmBw2epz"
      },
      "source": [
        "import experiment_tools.results as results\n",
        "import experiment_tools.load_dataset as experiment_dataset\n",
        "\n",
        "params = dataset_params + model_params\n",
        "results.print_params(params)\n",
        "\n",
        "folder, name = results.save_results_with_datetime(model, 'Vanilla', PATH, params)\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "Xtr_flat, Xtest_flat, Xvalid_flat = experiment_dataset.to_flatten_dataset(Xtr, Xtest, Xvalid)\n",
        "\n",
        "results.plot_valid_test_predictions(model, Xvalid_flat, Yvalid, Xtest_flat, Ytest, \n",
        "                                    y_feature=None, denorm_min=denorm_min, denorm_max=denorm_max, \n",
        "                                    folder=folder, base_name=name, model_returns_activations=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTCskCqW_jnE"
      },
      "source": [
        "## 3D CNN\n",
        "** Run on GPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtY7B5NI_kMc"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "import experiment_tools.load_dataset as load_dataset\n",
        "import experiment_tools.cnn3d_training as cnn3d_training\n",
        "\n",
        "# model\n",
        "epoch = 20\n",
        "patience = 20\n",
        "filters = 10\n",
        "kernel_size = 2\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "loss='mse'\n",
        "use_tpu = False\n",
        "\n",
        "model, model_params, history = cnn3d_training.train_model(dataset, \n",
        "                                                          epoch, patience,\n",
        "                                                          filters, kernel_size,\n",
        "                                                          batch_size, \n",
        "                                                          learning_rate, loss, use_tpu=use_tpu)\n",
        "\n",
        "end = time.time()\n",
        "print('Time elapsed: ', str(np.round(end - start, decimals=2)), 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptBxt1WbARQb"
      },
      "source": [
        "import experiment_tools.results as results\n",
        "import experiment_tools.cnn3d_training as cnn3d_training\n",
        "\n",
        "params = dataset_params + model_params\n",
        "results.print_params(params)\n",
        "\n",
        "folder, name = results.save_results_with_datetime(model, 'CNN3D', PATH, params)\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "Xtr_t, Xvalid_t, Xtest_t = cnn3d_training.transform_dataset(Xtr, Xvalid, Xtest)\n",
        "\n",
        "\n",
        "# y_feature=None, denorm_min=None, denorm_max=None,\n",
        "results.plot_valid_test_predictions(model, Xvalid_t, Yvalid, Xtest_t, Ytest, \n",
        "                                    y_feature=None, denorm_min=denorm_min, denorm_max=denorm_max, \n",
        "                                    folder=folder, base_name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIOhQSCaDBgI"
      },
      "source": [
        "## LSTM\n",
        "** Run on GPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHDdNc7rDBgM"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\n",
        "import experiment_tools.lstm_training as lstm_training\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# model\n",
        "epoch = 300\n",
        "patience = 20\n",
        "\n",
        "num_layers=2\n",
        "hidden_units=128\n",
        "dropout_rate=0.1\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "loss='mse'\n",
        "\n",
        "model, model_params = lstm_training.train_lstm(dataset,\n",
        "                                               epoch, patience,\n",
        "                                               num_layers, hidden_units, dropout_rate,\n",
        "                                               learning_rate, batch_size, loss)\n",
        "\n",
        "end = time.time()\n",
        "print('Time elapsed: ', str(np.round(end - start, decimals=2)), 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F85tG-uZDBgS"
      },
      "source": [
        "import experiment_tools.results as results\n",
        "import experiment_tools.load_dataset as experiment_dataset\n",
        "\n",
        "params = dataset_params + model_params\n",
        "results.print_params(params)\n",
        "\n",
        "folder, name = results.save_results_with_datetime(model, 'LSTM', PATH, params)\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "Xtr_flat, Xtest_flat, Xvalid_flat = experiment_dataset.to_flatten_dataset(Xtr, Xtest, Xvalid)\n",
        "\n",
        "\n",
        "# y_feature=None, denorm_min=None, denorm_max=None,\n",
        "r = results.plot_valid_test_predictions(model, Xvalid_flat, Yvalid, Xtest_flat, Ytest, \n",
        "                                    y_feature=None, denorm_min=denorm_min, denorm_max=denorm_max, \n",
        "                                    folder=folder, base_name=name)\n",
        "print(f'{r[2]}\\t{r[3]}\\t{r[4]}\\t{r[5]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRQXL-izB15z"
      },
      "source": [
        "## ConvLSTM\n",
        "** Run on GPU **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONowZCJbB153"
      },
      "source": [
        "import experiment_tools.load_dataset as load_dataset\n",
        "import experiment_tools.lstm_training as lstm_training\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "# model\n",
        "epoch = 300\n",
        "patience = 20\n",
        "\n",
        "num_layers=2\n",
        "filters=16\n",
        "kernel_size=3\n",
        "padding='same'\n",
        "dropout_rate=0.1\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "loss='mse'\n",
        "\n",
        "model, model_params = lstm_training.train_conv_lstm(dataset,\n",
        "                                                    epoch, patience,\n",
        "                                                    num_layers, filters, kernel_size, \n",
        "                                                    dropout_rate, padding,\n",
        "                                                    learning_rate, batch_size, loss)\n",
        "\n",
        "end = time.time()\n",
        "print('Time elapsed: ', str(np.round(end - start, decimals=2)), 'seconds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6WsOSOoB157"
      },
      "source": [
        "import experiment_tools.results as results\n",
        "import experiment_tools.lstm_training as lstm_training\n",
        "\n",
        "params = dataset_params + model_params\n",
        "results.print_params(params)\n",
        "\n",
        "folder, name = results.save_results_with_datetime(model, 'LSTM', PATH, params)\n",
        "\n",
        "Xtr, Ytr, Xvalid, Yvalid, Xtest, Ytest = dataset\n",
        "Xtr_flat, Xtest_flat, Xvalid_flat = lstm_training.transform_dataset_for_conv_lstm(Xtr, Xtest, Xvalid)\n",
        "\n",
        "\n",
        "# y_feature=None, denorm_min=None, denorm_max=None,\n",
        "r = results.plot_valid_test_predictions(model, Xvalid_flat, Yvalid, Xtest_flat, Ytest, \n",
        "                                    y_feature=None, denorm_min=denorm_min, denorm_max=denorm_max, \n",
        "                                    folder=folder, base_name=name)\n",
        "print(f'{r[2]}\\t{r[3]}\\t{r[4]}\\t{r[5]}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
